{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_file = '../Bank_list.xlsx'\n",
    "banks = pd.read_excel(bank_file)\n",
    "names = banks['Name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers[\"User-Agent\"] = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:29<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "for bank in tqdm(names):\n",
    "    for client_type in ['Individual', 'Corporation']:\n",
    "        folder_location = f'../bank_data/{client_type}/{bank}'\n",
    "        if not os.path.exists(folder_location):\n",
    "            os.makedirs(folder_location)\n",
    "        urls = banks[banks['Name'] == bank][client_type].dropna().tolist() \n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                    for link in soup.select(\"a[href$='.pdf']\"):\n",
    "                        try:\n",
    "                            # Name the pdf files using the last portion of each link which is unique in this case\n",
    "                            filename = os.path.join(folder_location, link['href'].split('/')[-1])\n",
    "                            with open(filename, 'wb') as f:\n",
    "                                pdf_content = requests.get(urljoin(url, link['href'])).content\n",
    "                                f.write(pdf_content)\n",
    "                        except (requests.RequestException, IOError) as e:\n",
    "                            print(f\"Error occurred while writing the file: {e}\")\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error occurred while fetching URL: {e}\")\n",
    "\n",
    "                #downloading websites\n",
    "                resp = requests.get(url)\n",
    "                html_content = resp.text\n",
    "\n",
    "                # Extract filename from URL or use a unique identifier\n",
    "                filename = url.split('/')[-1]  # Adjust this based on the URL structure\n",
    "                \n",
    "                # Modify the filename to make it unique within the directory\n",
    "                filepath = os.path.join(folder_location, f\"{filename}.html\")\n",
    "\n",
    "                # Write the HTML content to a file\n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(html_content)  # Write the raw HTML content directly to the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interestguardian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
